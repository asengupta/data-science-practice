{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Running the file<br>\n", "If you wish to use your own copy of the data, use the following command:<br>\n", "<br>\n", "``python bike-sharing-main.py [{-i |--input=}<bike-share-csv>] [-h | --help]``<br>\n", "<br>\n", "Here are some examples:<br>\n", "<br>\n", "``python bike-sharing-main.py --input=day.csv``<br>\n", "``python bike-sharing-main.py -i day.csv``<br>\n", "``python bike-sharing-main.py``<br>\n", "``python bike-sharing-main.py --help``<br>\n", "<br>\n", "All of these arguments are optional. Providing no arguments makes the code read from the default location, i.e. ```./data```.<br>\n", "<br>\n", "# Instructions on regenerating this Jupyter Notebook<br>\n", "The Jupyter notebook can be regenerated by installing P2J, like so:<br>\n", "<br>\n", "``pip install p2j``<br>\n", "<br>\n", "and running the following:<br>\n", "<br>\n", "``p2j -o code/bike-sharing-main.py -t notebook/bike-sharing-assignment-notebook.ipynb``"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Library Imports"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import getopt\n", "import logging\n", "import sys\n", "import warnings"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import seaborn as sns\n", "import statsmodels.api as sm\n", "from matplotlib import pyplot as plt\n", "from sklearn.feature_selection import RFE\n", "from sklearn.linear_model import LinearRegression\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import MinMaxScaler\n", "from statsmodels.stats.outliers_influence import variance_inflation_factor"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Constants<br>\n", "Setting up constants, so that magic strings are not repeated in the code"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["DEFAULT_DATASET_LOCATION = \"../data\"\n", "DEFAULT_BIKE_SHARE_CSV_FILENAME = \"day.csv\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class SeasonConstants:\n", "    SEASON_1 = \"spring\"\n", "    SEASON_2 = \"summer\"\n", "    SEASON_3 = \"fall\"\n", "    SEASON_4 = \"winter\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class WeatherConstants:\n", "    WEATHER_1 = \"Clear, Few clouds, Partly cloudy, Partly cloudy\"\n", "    WEATHER_2 = \"Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\"\n", "    WEATHER_3 = \"Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\"\n", "    WEATHER_4 = \"Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Setting up mapping of categorical levels to proper names to avoid confusion"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["SEASON_CATEGORICAL_MAPPING = {1: SeasonConstants.SEASON_1,\n", "                              2: SeasonConstants.SEASON_2,\n", "                              3: SeasonConstants.SEASON_3,\n", "                              4: SeasonConstants.SEASON_4}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["WEATHER_CATEGORICAL_MAPPING = {1: WeatherConstants.WEATHER_1,\n", "                               2: WeatherConstants.WEATHER_2,\n", "                               3: WeatherConstants.WEATHER_3,\n", "                               4: WeatherConstants.WEATHER_4}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Columns:\n", "    WORKING_DAY = \"workingday\"\n", "    HOLIDAY = \"holiday\"\n", "    SEASON = \"season\"\n", "    WEATHER = \"weathersit\"\n", "    DATE = \"dteday\"\n", "    DAY = \"day\"\n", "    INSTANT = \"instant\"\n", "    TEMPERATURE = 'temp'\n", "    FEELING_TEMPERATURE = 'atemp'\n", "    HUMIDITY = 'hum'\n", "    WINDSPEED = 'windspeed'\n", "    DAY_OF_WEEK = \"weekday\"\n", "    MONTH = \"mnth\"\n", "    CASUAL_COUNT = \"casual\"\n", "    REGISTERED_COUNT = \"registered\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["These will be the columns that will be scaled in the training set. The same scaling will be applied to the test set."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["COLUMNS_TO_SCALE = [Columns.TEMPERATURE, Columns.FEELING_TEMPERATURE,\n", "                    Columns.HUMIDITY, Columns.WINDSPEED, Columns.DAY,\n", "                    Columns.DAY_OF_WEEK, Columns.MONTH]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Null Column Cleanup<br>\n", "The loan dataset has several columns which are completely empty. These are useless for analysis.<br>\n", "This function drops completely null columns."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def without_null_columns(bikeshares):\n", "    heading(\"Null Entries Statistics\")\n", "    null_entry_statistics = bikeshares.isnull().sum() / len(bikeshares.index)\n", "    logging.info(null_entry_statistics)\n", "    null_columns = null_entry_statistics[null_entry_statistics == 1.0].index.to_numpy()\n", "    heading(\"Completely Null Columns\")\n", "    logging.info(null_columns)\n", "    return bikeshares.drop(null_columns, axis=1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Extract Day from Date"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def with_day(bikeshares):\n", "    dates = pd.to_datetime(bikeshares.pop(Columns.DATE), format='%d-%m-%Y')\n", "    log_df(\"Dates\", dates)\n", "    bikeshares[Columns.DAY] = pd.DatetimeIndex(dates).day\n", "    log_df(\"Bikeshares with Day\", bikeshares, 200)\n", "    return bikeshares"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exploratory Data Analysis<br>\n", "The function below performs some basic exploration of data to guide our search for relevant predictor variables. It does so by plotting correlations and heatmaps."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def explore(bikeshares):\n", "    plt.figure()\n", "    sns.pairplot(data=bikeshares)\n", "    plt.show()\n", "    plt.figure()\n", "    sns.heatmap(bikeshares.corr(), cmap=\"YlGnBu\", annot=True, annot_kws={\"size\": 5})\n", "    plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Initial Impressions<br>\n", "- `cnt` has a definite positive correlation with `temp`, `atemp`<br>\n", "- `cnt` has a definite positive correlation with `casual`<br>\n", "- `cnt` has a strong positive correlation with `registered`<br>\n", "- `cnt` has a correlation with `mnth`, but it is not linear<br>\n", "- If it's a holiday, `cnt` seems to be lower, considering higher percentiles"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Scaling of the dataset<br>\n", "This scaling occurs only once for the training dataset. The scaler is reused to scale the test data set."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def scale(bikeshares):\n", "    test_data_scaler = MinMaxScaler()\n", "    bikeshares[COLUMNS_TO_SCALE] = test_data_scaler.fit_transform(bikeshares[COLUMNS_TO_SCALE])\n", "    log_df(\"Bikeshares after Scaling\", bikeshares)\n", "    return bikeshares, test_data_scaler"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Training the Linear Regression Model<br>\n", "This trains the linear model using the training data set. Successively, the `atemp`, `month`, `day`, and `workingday` are dropped based on the combination of **Variance Inflation Factors** and **p-values**."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train(bikeshares_x_training, bikeshares_y_training):\n", "    lr = LinearRegression()\n", "    lr.fit(bikeshares_x_training, bikeshares_y_training)\n", "    bikeshares_x_training_0 = rfe_dummy(bikeshares_x_training, bikeshares_y_training, lr)\n", "    bikeshares_x_training_0 = sm.add_constant(bikeshares_x_training_0)\n\n", "    # Drop nothing in first iteration, just build model\n", "    lm_0, bikeshares_x_training_0 = simplify_model([], bikeshares_x_training_0,\n", "                                                   bikeshares_y_training)\n\n", "    # Drop `atemp`, it has high p-value and high VIF\n", "    lm_1, bikeshares_x_training_1 = simplify_model([Columns.FEELING_TEMPERATURE], bikeshares_x_training_0,\n", "                                                   bikeshares_y_training)\n\n", "    # Drop `month`, has high p-value and VIF~3.91\n", "    lm_2, bikeshares_x_training_2 = simplify_model([Columns.MONTH], bikeshares_x_training_1,\n", "                                                   bikeshares_y_training)\n\n", "    # Drop `day`, has high p-value but low VIF\n", "    lm_3, bikeshares_x_training_3 = simplify_model([Columns.DAY], bikeshares_x_training_2,\n", "                                                   bikeshares_y_training)\n\n", "    # Drop `workingday`, has high p-value > 0.005 but low VIF\n", "    lm_4, bikeshares_x_training_4 = simplify_model([Columns.WORKING_DAY], bikeshares_x_training_3,\n", "                                                   bikeshares_y_training)\n", "    return lm_4, bikeshares_x_training_4"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Utility function to make the dropping of columns, and retraining the model, less repetitive"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def simplify_model(columns_to_drop, bikeshares_x_training, bikeshares_y_training):\n", "    bikeshares_x_training_dropped = bikeshares_x_training.drop(columns_to_drop, axis=1)\n", "    log_df(\"#1 Bikeshare\", bikeshares_x_training_dropped)\n", "    linear_model = sm.OLS(bikeshares_y_training, bikeshares_x_training_dropped).fit()\n", "    summarise_model(bikeshares_x_training_dropped, linear_model)\n", "    return linear_model, bikeshares_x_training_dropped"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Utility function to summarise the model after every simplification of the Linear Model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def summarise_model(bikeshares_x_training, lm):\n", "    heading(\"Model Summary\")\n", "    logging.info(lm.summary())\n", "    vif_x(bikeshares_x_training)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Utility function to summarise the Variance Inflation Factors of the model after every simplification of the Linear Model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def vif_x(bikeshares_x_training):\n", "    vif = pd.DataFrame()\n", "    vif['Features'] = bikeshares_x_training.columns\n", "    vif['VIF'] = [variance_inflation_factor(bikeshares_x_training.values, i) for i in range(\n", "        bikeshares_x_training.shape[1])]\n", "    vif['VIF'] = round(vif['VIF'], 2)\n", "    vif = vif.sort_values(by=\"VIF\", ascending=False)\n", "    log_df('Variance Inflation Factors', vif)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Dummy function, which will not do Recursive Factor Elimination"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def rfe_dummy(bikeshares_x_training, bikeshares_y_training, lm):\n", "    return bikeshares_x_training"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Function, which performs Recursive Factor Elimination, if the automatic approach needs to be done in this code. Please note that the full reduction cycle of the linear model using this as the initial process, has not been tested."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def rfe_real(bikeshares_x_training, bikeshares_y_training, lm):\n", "    rfe = RFE(lm, 9)  # running RFE\n", "    rfe = rfe.fit(bikeshares_x_training, bikeshares_y_training)\n", "    print(list(zip(bikeshares_x_training.columns, rfe.support_, rfe.ranking_)))\n", "    relevant_columns_from_rfe = bikeshares_x_training.columns[rfe.support_]\n", "    heading(\"Selected columns from RFE\")\n", "    logging.info(relevant_columns_from_rfe)\n", "    heading(\"Dropped columns from RFE\")\n", "    logging.info(bikeshares_x_training.columns[~rfe.support_])\n", "    bikeshares_x_training_rfe = bikeshares_x_training[relevant_columns_from_rfe]\n", "    return bikeshares_x_training_rfe"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def predict_on_test_set(scaler, lm, bikeshares_test, bikeshares_x_training_reduced_columns):\n", "    bikeshares_y_test = bikeshares_test.pop('cnt')\n", "    bikeshares_x_test = bikeshares_test\n", "    bikeshares_x_test[COLUMNS_TO_SCALE] = scaler.transform(bikeshares_x_test[COLUMNS_TO_SCALE])\n", "    bikeshares_x_test = sm.add_constant(bikeshares_x_test)\n", "    bikeshares_x_test_reduced_columns = bikeshares_x_test[bikeshares_x_training_reduced_columns.columns]\n", "    bikeshares_y_predictions = lm.predict(bikeshares_x_test_reduced_columns)\n", "    fig = plt.figure()\n", "    plt.scatter(bikeshares_y_test, bikeshares_y_predictions)\n", "    fig.suptitle('y_test vs y_pred', fontsize=20)  # Plot heading\n", "    plt.xlabel('y_test', fontsize=18)  # X-label\n", "    plt.ylabel('y_pred', fontsize=16)  # Y-label\n", "    plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Verify Error Homoscedascity<br>\n", "This function plots the distribution of the error terms so that we may visually inspect whether the error terms (after applying the Linear Model to the Training data set) are normally distributed."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def verify_error_homoscedascity(bikeshares_x_training_reduced_columns, bikeshares_y_training, lm):\n", "    bikeshares_y_training_prediction = lm.predict(bikeshares_x_training_reduced_columns)\n", "    fig = plt.figure()\n", "    sns.distplot((bikeshares_y_training - bikeshares_y_training_prediction), bins=20)\n", "    fig.suptitle('Error Terms', fontsize=20)  # Plot heading\n", "    plt.xlabel('Errors', fontsize=18)  # X-label\n", "    plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Test Data / Training Data Split<br>\n", "This function splits the master data set into the Test and Training data sets for future model evaluation."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def test_train_split(bikeshares_master):\n", "    bikeshares_training, bikeshares_test = train_test_split(bikeshares_master, train_size=0.7, test_size=0.3,\n", "                                                            random_state=100)\n", "    heading(\"TEST / TRAIN SPLIT\")\n", "    logging.info(f\"Training set has {len(bikeshares_training)} vectors\")\n", "    logging.info(f\"Test set has {len(bikeshares_test)} vectors\")\n", "    return bikeshares_training, bikeshares_test"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Initial Data Preparation<br>\n", "This function is responsible for dropping unneeded columns, and converting categorical variables into dummy variables."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def prepared(bikeshares):\n", "    bikeshares_without_unneeded_columns = bikeshares.drop(\n", "        [Columns.INSTANT, Columns.CASUAL_COUNT, Columns.REGISTERED_COUNT], axis=1)\n", "    map_season = with_dummies_builder(Columns.SEASON, SEASON_CATEGORICAL_MAPPING)\n", "    map_weather = with_dummies_builder(Columns.WEATHER, WEATHER_CATEGORICAL_MAPPING)\n", "    return map_weather(map_season(with_day(bikeshares_without_unneeded_columns)))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def with_dummies_builder(categorical_column, category_mapping):\n", "    return lambda bikeshares: with_dummy_variables(bikeshares, categorical_column, category_mapping)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This function actually performs the dummy variable setup"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def with_dummy_variables(bikeshares, categorical_column, category_mapping):\n", "    seasons = pd.get_dummies(bikeshares.pop(categorical_column), drop_first=True)\n", "    log_df(f\"{categorical_column} before Renaming of Dummy Variables\", seasons)\n", "    seasons = seasons.rename(columns=category_mapping)\n", "    log_df(f\"{categorical_column} after Renaming of Dummy Variables\", seasons)\n", "    bikeshares_w_dummy_seasons = pd.concat([bikeshares, seasons], axis=1)\n", "    return bikeshares_w_dummy_seasons"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Problem-specific Utility Functions<br>\n", "Utility function to output the final Linear Model, together with a representation of the actual linear equation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def output_final_model(lm):\n", "    heading(\"FINAL MODEL\")\n", "    logging.info(lm.summary())\n", "    equation = regression_equation(lm)\n", "    heading(\"FINAL EQUATION\")\n", "    logging.info(equation)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Utility function to build the equation representation of the Linear Model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def regression_equation(lm):\n", "    terms = lm.params.index.map(lambda index: f\"({lm.params[index]} x {index})\")\n", "    equation = \" + \".join(terms)\n", "    return f\"Bike Rental Demand = {equation}\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Entry Point for the CRISP-DM process<br>\n", " This function is the entry point for the entire CRISPR process. This is called by `main()`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def study(raw_bike_share_data):\n", "    logging.debug(raw_bike_share_data.head().to_string())\n", "    logging.debug(raw_bike_share_data.shape)\n", "    logging.debug(raw_bike_share_data.columns)\n", "    bikeshares = without_null_columns(raw_bike_share_data)\n", "    log_df(\"Weather 4 data point does not exist in dataset\", bikeshares[bikeshares[Columns.WEATHER] == 4])\n", "    bikeshares_master = prepared(bikeshares)\n", "    log_df(\"BIKESHARES\", bikeshares_master, 10)\n", "    bikeshares_training, bikeshares_test = test_train_split(bikeshares_master)\n", "    bikeshares_training, scaler = scale(bikeshares_training)\n", "    explore(bikeshares_training)\n", "    bikeshares_y_training = bikeshares_training.pop('cnt')\n", "    bikeshares_x_training = bikeshares_training\n", "    lm, bikeshares_x_training_reduced_columns = train(bikeshares_x_training, bikeshares_y_training)\n", "    verify_error_homoscedascity(bikeshares_x_training_reduced_columns, bikeshares_y_training, lm)\n", "    predict_on_test_set(scaler, lm, bikeshares_test, bikeshares_x_training_reduced_columns)\n", "    output_final_model(lm)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Generic Application Utility Functions<br>\n", "This function reads command line arguments, one of which can be the input loan data set"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def parse_commandline_options(args):\n", "    print(f\"args are: {args}\")\n", "    loan_csv = f\"{DEFAULT_DATASET_LOCATION}/{DEFAULT_BIKE_SHARE_CSV_FILENAME}\"\n", "    try:\n", "        options, arguments = getopt.getopt(args, \"i:hf:\", [\"input=\", \"help\"])\n", "        for option, argument in options:\n", "            if option in (\"-h\", \"--help\"):\n", "                print_help_text()\n", "            elif option in (\"-i\", \"--input\"):\n", "                loan_csv = argument\n", "            else:\n", "                print(f\"{option} was not recognised as a valid option\")\n", "                print_help_text()\n", "                print(\"Allowing to continue since Jupyter notebook passes in other command-line options\")\n", "        return loan_csv\n", "    except getopt.GetoptError as e:\n", "        sys.stderr.write(\"%s: %s\\n\" % (args[0], e.msg))\n", "        print_help_text()\n", "        exit(2)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This function prints out the help text if either explicitly requested or in case of wrong input"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def print_help_text():\n", "    print(\"USAGE: python bike-sharing-main.py [{-i |--input=}<loan-csv>]\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This function overrides Jupyter's default logger so that we can output things based on our formatting preferences"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def setup_logging(log_level=logging.INFO):\n", "    warnings.filterwarnings(\"ignore\")\n", "    for handler in logging.root.handlers[:]:\n", "        logging.root.removeHandler(handler)\n", "    logger = logging.getLogger()\n", "    formatter = logging.Formatter('%(message)s')\n", "    ch = logging.StreamHandler()\n", "    ch.setLevel(log_level)\n", "    ch.setFormatter(formatter)\n", "    logger.setLevel(log_level)\n", "    logger.addHandler(ch)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This function reads the loan data set"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def read_csv(csv):\n", "    return pd.read_csv(csv, low_memory=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This utility function pretty prints a heading for output"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def heading(heading_text):\n", "    logging.info(\"-\" * 100)\n", "    logging.info(heading_text)\n", "    logging.info(\"-\" * 100)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This utility function pretty prints a dataframe for output"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def log_df(dataframe_label, dataframe, num_rows=10):\n", "    heading(dataframe_label)\n", "    logging.info(dataframe.head(num_rows).to_string())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Main Entry Point: main()<br>\n", "This function is the entry point of the script"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def main():\n", "    setup_logging()\n", "    study(read_csv(parse_commandline_options(sys.argv[1:])))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["main()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Conclusion<br>\n", "The below equation summarises the linear relationship between bike rental demand and the relevant predictor variables.<br>\n", "The selected predictors account for abput 83% of the variance in the data, as can be seen from the $R^2$ and the Adjusted $R^2$ statistics listed."]}, {"cell_type": "markdown", "metadata": {}, "source": ["$\\mathbf{rental\\_bike\\_demand} = 1632.382\\times \\mathbf{C} + 2019.478\\times \\mathbf{year} - 667.709\\times \\mathbf{holiday} + 415.293\\times \\mathbf{weekday} + 4298.769\\times \\mathbf{temperature} - 1082.769\\times \\mathbf{humidity} -1583.394\\times \\mathbf{windspeed} + 1025.765\\times \\mathbf{summer} + 647.160\\times \\mathbf{fall} + 1425.0694\\times \\mathbf{winter} -501.458\\times \\mathbf{WEATHER\\_2} -2150.311\\times \\mathbf{WEATHER\\_3}$<br>\n", "where:<br>\n", "**WEATHER_2** = \"Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist)\"<br>\n", "**WEATHER_3** = \"Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\"<br>\n", "and the other variables are appropriately scaled."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}