# # Running the file
# If you wish to use your own copy of the data, use the following command:
#
# ``python bike-sharing-main.py [{-i |--input=}<bike-share-csv>] [-h | --help]``
#
# Here are some examples:
#
# ``python bike-sharing-main.py --input=day.csv``
# ``python bike-sharing-main.py -i day.csv``
# ``python bike-sharing-main.py``
# ``python bike-sharing-main.py --help``
#
# All of these arguments are optional. Providing no arguments makes the code read from the default location, i.e. ```./data```.
#
# # Instructions on regenerating this Jupyter Notebook
# The Jupyter notebook can be regenerated by installing P2J, like so:
#
# ``pip install p2j``
#
# and running the following:
#
# ``p2j -o code/bike-sharing-main.py -t notebook/bike-sharing-main.ipynb``

# # Library Imports
import getopt
import logging
import sys
import warnings

import pandas as pd
import statsmodels.api as sm
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from statsmodels.stats.outliers_influence import variance_inflation_factor

DEFAULT_DATASET_LOCATION = "../data"
DEFAULT_BIKE_SHARE_CSV_FILENAME = "day.csv"


class SeasonConstants:
    SEASON_1 = "spring"
    SEASON_2 = "summer"
    SEASON_3 = "fall"
    SEASON_4 = "winter"


class WeatherConstants:
    WEATHER_1 = "Clear, Few clouds, Partly cloudy, Partly cloudy"
    WEATHER_2 = "Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist"
    WEATHER_3 = "Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds"
    WEATHER_4 = "Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog"


SEASON_CATEGORICAL_MAPPING = {1: SeasonConstants.SEASON_1,
                              2: SeasonConstants.SEASON_2,
                              3: SeasonConstants.SEASON_3,
                              4: SeasonConstants.SEASON_4}

WEATHER_CATEGORICAL_MAPPING = {1: WeatherConstants.WEATHER_1,
                               2: WeatherConstants.WEATHER_2,
                               3: WeatherConstants.WEATHER_3,
                               4: WeatherConstants.WEATHER_4}


class Columns:
    HOLIDAY = "holiday"
    SEASON = "season"
    WEATHER = "weathersit"
    DATE = "dteday"
    DAY = "day"
    INSTANT = "instant"
    TEMPERATURE = 'temp'
    FEELING_TEMPERATURE = 'atemp'
    HUMIDITY = 'hum'
    WINDSPEED = 'windspeed'
    DAY_OF_WEEK = "weekday"
    MONTH = "mnth"
    CASUAL_COUNT = "casual"
    REGISTERED_COUNT = "registered"


COLUMNS_TO_SCALE = [Columns.TEMPERATURE, Columns.FEELING_TEMPERATURE,
                    Columns.HUMIDITY, Columns.WINDSPEED, Columns.DAY,
                    Columns.DAY_OF_WEEK, Columns.MONTH]


# ## Null Column Cleanup
# The loan dataset has several columns which are completely empty. These are useless for analysis.
# This function drops completely null columns.
def without_null_columns(bikeshares):
    heading("Null Entries Statistics")
    null_entry_statistics = bikeshares.isnull().sum() / len(bikeshares.index)
    logging.info(null_entry_statistics)
    null_columns = null_entry_statistics[null_entry_statistics == 1.0].index.to_numpy()
    heading("Completely Null Columns")
    logging.info(null_columns)
    return bikeshares.drop(null_columns, axis=1)


def with_day(bikeshares):
    dates = pd.to_datetime(bikeshares.pop(Columns.DATE), format='%d-%m-%Y')
    log_df("Dates", dates)
    bikeshares[Columns.DAY] = pd.DatetimeIndex(dates).day
    log_df("Bikeshares with Day", bikeshares, 200)
    return bikeshares


# # Initial Observations
# - `cnt` has a definite positive correlation with `temp`, `atemp`
# - `cnt` has a definite positive correlation with `casual`
# - `cnt` has a strong positive correlation with `registered`
# - `cnt` has a correlation with `mnth`, but it is not linear
# - If it's a holiday, `cnt` seems to be lower, considering higher percentiles
def explore(bikeshares):
    # plt.figure()
    # sns.pairplot(data=bikeshares)
    # plt.show()
    pass


def scale(bikeshares):
    test_data_scaler = MinMaxScaler()
    bikeshares[COLUMNS_TO_SCALE] = test_data_scaler.fit_transform(bikeshares[COLUMNS_TO_SCALE])
    log_df("Bikeshares after Scaling", bikeshares)
    return bikeshares, test_data_scaler


def train(bikeshares_x_training, bikeshares_y_training):
    lm_0 = LinearRegression()
    lm_0.fit(bikeshares_x_training, bikeshares_y_training)
    bikeshares_x_training_rfe_0 = rfe_dummy(bikeshares_x_training, bikeshares_y_training, lm_0)
    bikeshares_x_training_rfe_0 = sm.add_constant(bikeshares_x_training_rfe_0)
    lm_0 = sm.OLS(bikeshares_y_training, bikeshares_x_training_rfe_0).fit()  # Running the linear model

    summarise_model(bikeshares_x_training_rfe_0, lm_0)

    # Drop `atemp`, it has high p-value and high VIF
    bikeshares_x_training_rfe_1 = bikeshares_x_training_rfe_0.drop([Columns.FEELING_TEMPERATURE], axis = 1)
    log_df("#1 Bikeshare", bikeshares_x_training_rfe_1)
    lm_1 = sm.OLS(bikeshares_y_training, bikeshares_x_training_rfe_1).fit()  # Running the linear model

    summarise_model(bikeshares_x_training_rfe_1, lm_1)

    # Drop `month`, has high p-value and VIF~3.91
    bikeshares_x_training_rfe_2 = bikeshares_x_training_rfe_1.drop([Columns.MONTH], axis = 1)
    log_df("#2 Bikeshare", bikeshares_x_training_rfe_2)
    lm_2 = sm.OLS(bikeshares_y_training, bikeshares_x_training_rfe_2).fit()  # Running the linear model

    summarise_model(bikeshares_x_training_rfe_2, lm_2)

    # Drop `day`, has high p-value but low VIF
    bikeshares_x_training_rfe_3 = bikeshares_x_training_rfe_2.drop([Columns.DAY], axis = 1)
    log_df("#3 Bikeshare", bikeshares_x_training_rfe_3)
    lm_3 = sm.OLS(bikeshares_y_training, bikeshares_x_training_rfe_3).fit()  # Running the linear model

    summarise_model(bikeshares_x_training_rfe_3, lm_3)

    # Drop `workingday`, has high p-value > 0.005 but low VIF
    bikeshares_x_training_rfe_4 = bikeshares_x_training_rfe_3.drop([Columns.HOLIDAY], axis = 1)
    log_df("#4 Bikeshare", bikeshares_x_training_rfe_4)
    lm_4 = sm.OLS(bikeshares_y_training, bikeshares_x_training_rfe_4).fit()  # Running the linear model

    summarise_model(bikeshares_x_training_rfe_4, lm_4)


def summarise_model(bikeshares_x_training, lm):
    heading("Model Summary")
    logging.info(lm.summary())
    vif_x(bikeshares_x_training)


def vif_x(bikeshares_x_training):
    vif = pd.DataFrame()
    vif['Features'] = bikeshares_x_training.columns
    vif['VIF'] = [variance_inflation_factor(bikeshares_x_training.values, i) for i in range(
        bikeshares_x_training.shape[1])]
    vif['VIF'] = round(vif['VIF'], 2)
    vif = vif.sort_values(by="VIF", ascending=False)
    log_df('Variance Inflation Factors', vif)


def rfe_dummy(bikeshares_x_training, bikeshares_y_training, lm):
    return bikeshares_x_training


def rfe_real(bikeshares_x_training, bikeshares_y_training, lm):
    rfe = RFE(lm, 9)  # running RFE
    rfe = rfe.fit(bikeshares_x_training, bikeshares_y_training)
    print(list(zip(bikeshares_x_training.columns, rfe.support_, rfe.ranking_)))
    relevant_columns_from_rfe = bikeshares_x_training.columns[rfe.support_]
    heading("Selected columns from RFE")
    logging.info(relevant_columns_from_rfe)
    heading("Dropped columns from RFE")
    logging.info(bikeshares_x_training.columns[~rfe.support_])
    bikeshares_x_training_rfe = bikeshares_x_training[relevant_columns_from_rfe]
    return bikeshares_x_training_rfe


# # Entry Point for CRISPR
#  This function is the entry point for the entire CRISPR process. This is called by `main()`
def study(raw_bike_share_data):
    logging.debug(raw_bike_share_data.head().to_string())
    logging.debug(raw_bike_share_data.shape)
    logging.debug(raw_bike_share_data.columns)
    bikeshares = without_null_columns(raw_bike_share_data)
    log_df("Weather 4 data point does not exist in dataset", bikeshares[bikeshares[Columns.WEATHER] == 4])
    bikeshares_master = prepared(bikeshares)
    log_df("BIKESHARES", bikeshares_master, 10)
    bikeshares_training, bikeshares_test = test_train_split(bikeshares_master)
    bikeshares_training, scaler = scale(bikeshares_training)
    explore(bikeshares_training)

    bikeshares_y_training = bikeshares_training.pop('cnt')
    bikeshares_x_training = bikeshares_training
    train(bikeshares_x_training, bikeshares_y_training)


def test_train_split(bikeshares_master):
    bikeshares_training, bikeshares_test = train_test_split(bikeshares_master, train_size=0.7, test_size=0.3,
                                                            random_state=100)
    heading("TEST / TRAIN SPLIT")
    logging.info(f"Training set has {len(bikeshares_training)} vectors")
    logging.info(f"Test set has {len(bikeshares_test)} vectors")
    return bikeshares_training, bikeshares_test


def prepared(bikeshares):
    # Registered and casual counts should not be used to determine total demand, because they are dependent variables as well,
    # Separate analysis needs to be done to determine dependency of these values on the factors.
    bikeshares_without_unneeded_columns = bikeshares.drop(
        [Columns.INSTANT, Columns.CASUAL_COUNT, Columns.REGISTERED_COUNT], axis=1)
    map_season = with_dummies_builder(Columns.SEASON, SEASON_CATEGORICAL_MAPPING)
    map_weather = with_dummies_builder(Columns.WEATHER, WEATHER_CATEGORICAL_MAPPING)
    return map_weather(map_season(with_day(bikeshares_without_unneeded_columns)))


def with_dummies_builder(categorical_column, category_mapping):
    return lambda bikeshares: with_dummy_variables(bikeshares, categorical_column, category_mapping)


def with_dummy_variables(bikeshares, categorical_column, category_mapping):
    seasons = pd.get_dummies(bikeshares.pop(categorical_column), drop_first=True)
    log_df(f"{categorical_column} before Renaming of Dummy Variables", seasons)
    seasons = seasons.rename(columns=category_mapping)
    log_df(f"{categorical_column} after Renaming of Dummy Variables", seasons)
    bikeshares_w_dummy_seasons = pd.concat([bikeshares, seasons], axis=1)
    return bikeshares_w_dummy_seasons


# # Utility Functions
# This function reads command line arguments, one of which can be the input loan data set
def parse_commandline_options(args):
    print(f"args are: {args}")
    loan_csv = f"{DEFAULT_DATASET_LOCATION}/{DEFAULT_BIKE_SHARE_CSV_FILENAME}"

    try:
        options, arguments = getopt.getopt(args, "i:hf:", ["input=", "help"])
        for option, argument in options:
            if option in ("-h", "--help"):
                print_help_text()
            elif option in ("-i", "--input"):
                loan_csv = argument
            else:
                print(f"{option} was not recognised as a valid option")
                print_help_text()
                print("Allowing to continue since Jupyter notebook passes in other command-line options")
        return loan_csv
    except getopt.GetoptError as e:
        sys.stderr.write("%s: %s\n" % (args[0], e.msg))
        print_help_text()
        exit(2)


# This function prints out the help text if either explicitly requested or in case of wrong input
def print_help_text():
    print("USAGE: python bike-sharing-main.py [{-i |--input=}<loan-csv>]")


# This function overrides Jupyter's default logger so that we can output things based on our formatting preferences
def setup_logging():
    warnings.filterwarnings("ignore")
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    logger = logging.getLogger()
    formatter = logging.Formatter('%(message)s')
    ch = logging.StreamHandler()
    ch.setLevel(logging.DEBUG)
    ch.setFormatter(formatter)
    logger.setLevel(logging.DEBUG)
    logger.addHandler(ch)


# This function reads the loan data set
def read_csv(csv):
    return pd.read_csv(csv, low_memory=False)


# This utility function pretty prints a heading for output
def heading(heading_text):
    logging.info("-" * 100)
    logging.info(heading_text)
    logging.info("-" * 100)


# This utility function pretty prints a dataframe for output
def log_df(dataframe_label, dataframe, num_rows=10):
    heading(dataframe_label)
    logging.info(dataframe.head(num_rows).to_string())


# # Main Entry Point: main()
# This function is the entry point of the script
def main():
    setup_logging()
    study(read_csv(parse_commandline_options(sys.argv[1:])))


main()
